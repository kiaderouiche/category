TODO: Adjust the following lines from README.md

# LightSeq: A High Performance Inference Library for Sequence Processing and Generation
![logo](./docs/images/logo.png)


LightSeq is a high performance inference library for sequence processing and generation implemented
in CUDA. 
It enables highly efficient computation of modern NLP models such as **BERT**, **GPT2**,
**Transformer**, etc. 
It is therefore best useful for *Machine Translation*, *Text Generation*, *Dialog*ï¼Œ *Language
Modelling*, and other related tasks using these models. 

The library is built on top of CUDA official
library([cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html),
[Thrust](https://docs.nvidia.com/cuda/thrust/index.html), [CUB](http://nvlabs.github.io/cub/)) and
custom kernel functions which are specially fused and optimized for these widely used models. In
addition to model components, we also provide codes
manage model weights trained from deepleanring framework and servers as a custom backend for
[TensorRT Inference
Server](https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/tensorrt_inference_server_120/tensorrt-inference-server-guide/docs/quickstart.html)(referred
to as TRTIS in the later discussion). With LightSeq, you can easily deploy efficient model services or develop
...
